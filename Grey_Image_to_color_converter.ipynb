{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!wget http://data.csail.mit.edu/places/places205/testSetPlaces205_resize.tar.gz\n",
    "!tar -xzf testSetPlaces205_resize.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('Color_Images/Train/color_data/', exist_ok=True) # 40,000 images\n",
    "os.makedirs('Color_Images/Test/color_data/', exist_ok=True)   #  1,000 images\n",
    "for i, file in enumerate(os.listdir('testSet_resize')):\n",
    "  if i < 1000: # first 1000 will be val\n",
    "    os.rename('testSet_resize/' + file, 'Color_Images/Test/color_data/' + file)\n",
    "  else: # others will be val\n",
    "    os.rename('testSet_resize/' + file, 'Color_Images/Train/color_data/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(os.listdir('Color_Images/Train/color_data/'))       # 40000 training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the images are there\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='Color_Images/Train/color_data/b13dcc2414fde1747442b4d068148a12.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
    "from skimage import io\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import os, shutil, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_counter = 0\n",
    "for child in model.children():\n",
    "    print(\" child\", child_counter, \"is -\")\n",
    "    print(child)\n",
    "    child_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Checkpoints', exist_ok=True)     # To save the checkpoints created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Color_Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayscaleImageFolder(datasets.ImageFolder):\n",
    "  '''Custom images folder, which converts images to grayscale before loading'''\n",
    "  def __getitem__(self, index):\n",
    "    path, target = self.imgs[index]\n",
    "    img = self.loader(path)\n",
    "    if self.transform is not None:\n",
    "      img_original = self.transform(img)\n",
    "      img_original = np.asarray(img_original)\n",
    "      img_lab = rgb2lab(img_original)\n",
    "      img_lab = (img_lab + 128) / 255       #128 is added because the A & B channels have values in range -128 to 128.\n",
    "      img_ab = img_lab[:, :, 1:3]           # We want only the AB channels.\n",
    "      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "      img_gray = rgb2gray(img_original) \n",
    "      #The rgb2gray conversion removes the channel dimension, so the output dimension will be (224,224).\n",
    "      img_gray = torch.from_numpy(img_gray).unsqueeze(0).float() #Thats why we add a singleton dimension.\n",
    "    if self.target_transform is not None:\n",
    "      target = self.target_transform(target)\n",
    "    return img_original, img_gray, img_ab, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(300),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                #transforms.RandomRotation(30),\n",
    "                                transforms.RandomVerticalFlip(),  \n",
    "                                #transforms.ToTensor()\n",
    "                                #transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                #                     [0.5, 0.5, 0.5])\n",
    "                               ])\n",
    "test_transform = transforms.Compose([transforms.Resize(300),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                #transforms.ToTensor()\n",
    "                                #transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                #                     [0.5, 0.5, 0.5])\n",
    "                                ])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = GrayscaleImageFolder(root+'Train', transform=train_transform)\n",
    "test_data = GrayscaleImageFolder(root+'Test', transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(color_original, inp_gray, reconstructed_color):\n",
    "    gray = np.transpose(inp_gray, (1, 2, 0))\n",
    "    gray =  np.squeeze(gray, axis=(2,))\n",
    "    \n",
    "    fig=plt.figure(figsize=[15,5])\n",
    "    \n",
    "    # Normalizing\n",
    "    #orig     = (color_original - color_original.min()) / (color_original.max() - color_original.min())\n",
    "    #gray    = (gray - gray.min()) / (gray.max() - gray.min())\n",
    "    #reconstructed_color = (reconstructed_color - reconstructed_color.min()) / (reconstructed_color.max() - reconstructed_color.min())\n",
    "    \n",
    "    fig.add_subplot(1, 3, 1, title='Original color')\n",
    "    plt.imshow(color_original)\n",
    "    \n",
    "    fig.add_subplot(1, 3, 2, title='Gray')\n",
    "    plt.imshow(gray, cmap = 'gray')\n",
    "    \n",
    "    fig.add_subplot(1, 3, 3, title='Reconstructed color')\n",
    "    plt.imshow(reconstructed_color)\n",
    "    \n",
    "    fig.subplots_adjust(wspace = 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(ab_img, gray_img):\n",
    "    plt.clf() # clear matplotlib \n",
    "    color_image = torch.cat((gray_img, ab_img), 0).numpy() # combine channels\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
    "    color_image = lab2rgb(color_image.astype(np.float64))\n",
    "    \n",
    "    return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colarization(nn.Module):\n",
    "    def __init__(self, input_size=128):\n",
    "        super(Colarization, self).__init__()\n",
    "    \n",
    "        ## First half: Encoding\n",
    "        resnet = models.resnet18(num_classes=365) \n",
    "        # Change first conv layer to accept single-channel (grayscale) input\n",
    "        resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
    "        # Extract midlevel features from ResNet-gray\n",
    "        self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
    "\n",
    "        ## Second half: Decoding (Upsampling)\n",
    "        self.upsample = nn.Sequential(     \n",
    "        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2),\n",
    "        nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2),\n",
    "        nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
    "        nn.Upsample(scale_factor=2)\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass input through ResNet-gray to extract features\n",
    "        #midlevel_features = self.midlevel_resnet(input)\n",
    "        x = self.midlevel_resnet(x)\n",
    "        # Upsample to get colors\n",
    "        output = self.upsample(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Colarization()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001, weight_decay=0.0)    #weight_decay=4e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "# Training the model \n",
    "\n",
    "epochs = 120\n",
    "#steps = 0\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_iter = 0\n",
    "    print('****************************************')\n",
    "    print('Starting epoch:',e+1)\n",
    "    for i, (color_img, input_gray, input_ab, target) in enumerate(trainloader):\n",
    "        input_gray = input_gray.to(device)\n",
    "        input_ab = input_ab.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output_ab = autoencoder(input_gray) \n",
    "        #print(output.shape)\n",
    "        loss = criterion(output_ab, input_ab)           \n",
    "        loss.backward()              # For gradient calcultion\n",
    "        optimizer.step()             # Optimizng - Tuning the weights of the model\n",
    "    \n",
    "        running_iter +=1\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    test_loss = 0\n",
    "    test_iter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (color_img, input_gray, input_ab, target) in enumerate(testloader):\n",
    "            \n",
    "            input_gray = input_gray.to(device)\n",
    "            input_ab = input_ab.to(device)\n",
    "            \n",
    "            output_ab = autoencoder(input_gray)\n",
    "            loss = criterion(output_ab, input_ab)\n",
    "        \n",
    "            test_iter +=1\n",
    "            test_loss += loss.item()\n",
    "      \n",
    "    \n",
    "    # Visualizing the first image of the last batch in the validation set\n",
    "    #original_gray = input_gray.cpu()\n",
    "    inp_gray = input_gray[0].cpu()\n",
    "    out_ab = output_ab[0].cpu()\n",
    "    \n",
    "    color_img = color_img[0].data.numpy()\n",
    "    in_gray = inp_gray.data.numpy()\n",
    "    ot_ab = out_ab.data.numpy()\n",
    "    \n",
    "    #print('input gray:',inp_gray.shape)\n",
    "    #print('output ab channels:',out_ab.shape)\n",
    "    print(\"Epoch:\",e+1)\n",
    "    print('Train loss:',running_loss)\n",
    "    print('Test loss:',test_loss)\n",
    "    \n",
    "    reconstructed_color = to_rgb(out_ab, inp_gray)\n",
    "    \n",
    "    #print('input gray:',inp_gray.shape)\n",
    "    #print('output ab channels:',out_ab.shape)\n",
    "    #print('reconstructed color:',reconstructed_color.shape)\n",
    "    \n",
    "    show_img(color_img, in_gray, reconstructed_color)\n",
    "    train_loss.append(running_loss / running_iter)\n",
    "    valid_loss.append(test_loss / test_iter)\n",
    "    test_loss = 0\n",
    "    running_loss = 0\n",
    "    if((e+1)%15 == 0):                    # Saving model every 15 epochs\n",
    "        print('Saving model at epoch:',e)\n",
    "        torch.save(autoencoder.state_dict(), 'Checkpoints/checkpoint_colorize_'+str(e)+'.pth')\n",
    "    autoencoder.train()\n",
    "    print('****************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
